\section{Running \PDAC}
\label{section:run}

PDAC runs on a variety of serial and parallel platforms.  While it is
trivial to launch a serial program, a parallel program depends on a
platform-specific library such as MPI to launch copies of itself on
other nodes and to provide access to a high performance network such
as Myrinet if one is available.

On all parallel systems, either cluster of workstations or 
parallel supercomputers, \PDAC\ relais on the local implementation
of MPI for its communications, and standard system tools such as
mpirun are used to launch jobs. Therefore a working installation
of MPI is a prerequisite in order to run \PDAC\ .
The \PDAC\ serial binaries can also be
run directly (known as standalone mode) for single process runs,
on most platforms.
Since MPI libraries are very often
incompatible between versions, you will likely need to recompile \PDAC\
to use these machines in parallel
(the provided non-MPI binaries should still work for serial runs.)

\subsection{Individual Unix Workstations}

In this section we will explain how to run \PDAC\ on your own
workstation without a queuing system like
OpenPBS or Loadleveler ( see below on this chapter ).
On individual Unix workstations the use of serial \PDAC\ is
quite easy, basically it behaves like any other Unix process
running on your machine.
To run \PDAC\ then follow the steps:

\begin{itemize}

\item create a running directory\
\begin{verbatim}
  mkdir sample_run
\end{verbatim}

\item copy executable (pdac.x) and input file (pdac.dat)
      to the running directory\
\begin{verbatim}
  cp pdac.x pdac.dat sample_run
\end{verbatim}

\item run pdac.x ( possibly in background )
\begin{verbatim}
  pdac.x &
\end{verbatim}

\end{itemize}

Nevertheless you are the only user of your workstation,
we suggest to brokeup the simulation in shorter partial
simulations ( few days maximum each ) and use the
restart capability od \PDAC. In this way you 
reduce the probability of data/work loss due to 
unforseenable events (a system crash, a blackout, ecc...).
When restarting a simulation, remember to set the 
restart mode to "restart" and to save the pdac.res file,
in case somthing goes wrong in the new simulation step.

For multiprocessor workstations, if you have a working MPI
library, you can compile \PDAC\ in parallel mode to use
all processors of the workstations. In this case,
to run \PDAC\ in parallel you should use an MPI loader 
(usually distributed with the MPI library), i.e. if
your MPI library is MPICH you can run \PDAC\ using the commad:

\begin{verbatim}
  mpirun -np 2 pdac.x
\end{verbatim}

where the command parameter "-np 2" specify that you want
to run on two processors.

\subsection{Server and Parallel Supercomputers}

In this section we explain how to run \PDAC\ on Unix servers
and parallel supercomputing, giving you some practical example
on most popular architectures (at the time of writing).
The main differences with respect running on individual workstations
is represented by the presence of a queuing system that
manage the users rtequests. Usually you should also 
consider the limits the administrator has set to your user,
like disk space, memory or available running que. 
If this limits are not taken in to account could cause
the simulation not to run or complete.
Below, as anticipated, we give you few example on how to run
on popular server and supercomputers, but remember that 
this system are highly customizable and what is presented 
probably should be modified to run on other supercomputers.

\subsubsection{IBM RS/6000 servers and parallel supercomputers}

On most of IBM servers and RS/6000 supercomputers, to run a simulation you
should write a job script to be submitted to the Loadleveler which
is the IBM queuing system.
Many systems allows you to run MPI program in interactive mode for
testing and debug, through the POE program.  
The options and environment variables for poe are various and arcane, 
so you should consult your local documentation for recommended settings.  
As an example for interactive mode run on the RS/6000 supercomputer
(Sp4) installed at CINECA at the shell prompt you should give the
command:

\begin{verbatim}
  poe ./pdac.x -procs 4 -nodes 1 
\end{verbatim}

to run on a single node using 4 procs.
For long run and simulation it is required that you write a job script,
and submit it to the loadleveler, here there is an example.

\begin{verbatim}
#!/bin/ksh
#@ job_type = parallel
#@ output = job.out
#@ error = job.err
#@ notification = never
#@ checkpoint = no
#@ restart = no
#@ wall_clock_limit =6:00:00
#@ resources = ConsumableCpus(1) ConsumableMemory(1024 mb)
#@ class = parallel
#@ network.MPI = csss,shared,US
#@ total_tasks = 8
#@ blocking = unlimited
#@ queue

cp pdac.dat /scratch/myrundir/pdac.dat
cp pdac.x /scratch/myrundir/pdac.x
cd /scratch/myrundir/
./pdac.x
\end{verbatim}

The above script ask for 8 processors to run 8 MPI tasks
with a limit of 1024Mbyte each for 6 hours on the parallel queue.
Then you should submit the job (suppose its filename is myjobfile):

\begin{verbatim}
llsubmit myjobfile
\end{verbatim}

\subsubsection{Linux Clusters (Beowulf)}

The most common way to run program on a linux cluster 
is through the use of the combination of OpenPBS, MAUI scheduler
and mpirun/mpiexec MPI loaders .
For this kind of machine you could find an almost infinite number
of environments and system softwares, therefore keep the example below
as an indication, most probably it would not work on other systems. 
As for other server you need to write a job script with
the request for the queuing system and with the command you 
want to execute.

\begin{verbatim}
#!/bin/sh
#PBS -l nodes=16:ppn=2,walltime=6:00:00

cp pdac.dat /myrundir
cp pdac.x /myrundir
cd /myrundir
mpiexec -no-shmem -np 32 ./pdac.x
\end{verbatim}

with this script we are asking 16 nodes with 2 processor per node
and 6 hours of execution time in the queue. Then you need to
submit the job to the queuing system, for OpenPBS the command is:

\begin{verbatim}
qsub myjobscript
\end{verbatim}

here we have supposed that the filename of the above script is
myjobscript.

\subsubsection{Compaq AlphaServer SC}

If your machine as a Quadrics interconnect you should use the 
Elan communication subsystem commands to run your parallel applications
or jobs. The main command is the ``prun'' command, that is similar
to mpirun of MPICH, but with prun you could run both job scripts
and interactive commands. Here there is an example :

\begin{verbatim}
prun -n 8 ./pdac.x
\end{verbatim}

this command will run pdac.x on 8 processors, as soon as they will
be free, infact if there are other job (or commands) already running
your command will be queued.
There are additional options.  Consult your local documentation.

\subsubsection{SGI Origin systems}

SGI Origin system are shared memory parallel machines, with
real single system image framework, in other words the user
could see or access all the processors directly from the 
login environment. At variance with othe architectures,
there is no distinction between login and execution node.
This as some benefits and drowback, the benefits come from
the possibility to use the whole machine as a multiprocessors
workstation with an high number of processors, the drowback
are related to the possibility that a different jobs running
at the same time on the machine interfere with each others.
In general you could run then parallel applications 
in two different way, using a shell command like in your own
workstation and using a queuing system, which is usually NQS.
To run from the shell prompt use mpirun, i.e.: 

\begin{verbatim}
  mpirun -np 8 ./pdac.x
\end{verbatim}

this will execute pdac.x with 8 MPI task, that not necessarily
goes to different processors, it depends on system loads.
Although this way of execute parallel applications is very
friendly, it could cause a lot of interferences with system
activities and other applications, with the result of slowing
down the system. Then quite often the system administrator
set severe limits to this way of run, mainly on the number of
real available processors (no matter how many MPI task you ask for) 
execution time and memory usage.
To run large application then on many system you need to 
interface with the NQS queuing system, then as for other architectures
you have to write a script with the requests for processors and
memory and the submit the script to NQS.
An example of job script for NQS is 

\begin{verbatim}
#!/bin/sh
#QSUB -l mpp_p=8
#QSUB -l p_mpp_t=2:00:00

cp pdac.dat /myrundir
cp pdac.x /myrundir
cd /myrundir
mpirun -np 8 ./pdac.x
\end{verbatim}

with this script you ask 8 processors for 2 hours, then
you could submit the script with 
(suppose its filename is myjobfile):

\begin{verbatim}
  qsub myjobfile
\end{verbatim}

\subsection{Grid environment}

\subsection{Memory Usage}

.... Memory usage consideration ...

\subsection{Improving Parallel Scaling}

.... Choosing the correct number of processors

While \PDAC\ is designed to be a scalable program, particularly for
large simulations ( 100,000 cells or more), at some point adding additional
processors to a simulation will provide little or no extra performance.
If you are lucky enough to have access to a parallel machine you should
measure \PDAC\'s parallel speedup for a variety of processor counts when
running your particular simulation.  The easiest and most accurate way
to do this is to look at the ``Prog'' time printed in the "pdac.dat" file.
You can monitor performance during the entire simulation looking at
the "walltime" line in the "pdac.log" file.

Extremely short cycle lengths (less than 10 steps) will also limit
parallel scaling, since the ghost cells migration at the end of each cycle
sends too many messages.
