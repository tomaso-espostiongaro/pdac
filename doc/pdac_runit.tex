\section{Running \PDAC}
\label{section:run}

\subsection{Getting started: what is needed}

Before running \PDAC\ the following elements are needed:
\begin{itemize}
\item The \PDAC\ executable \FIL{pdac.x}
\item The \PDAC\ parameter file \FIL{pdac.dat}.
\end{itemize}

Other files (such as the topography DEM or the restart file) can be needed,
depending on the input parameters (see Sect. \ref{section:input_par}).

Here it is assumed that you either succeded in compiling the source
code or that you got a precompiled executable suited for your architecture.
Be sure to have the authorization for running the executable on the
directory where it is installed. For further informations on compiling and 
on code portability, see Sect. \ref{section:avail}.\\

PDAC runs on a variety of serial and parallel platforms.  While it is
trivial to launch a serial program, the execution of a parallel program 
depends on a platform-specific library, such as MPI, that allows to launch 
copies of the executable on other nodes and to provide access to a high 
performance network such as Myrinet (if available) or to a Ethernet connection.

On all parallel systems, either cluster of workstations or parallel 
supercomputers, \PDAC\ execution is relied to the local implementation
of MPI for data communications, and to standard system tools such as
{\tt mpirun} to launch jobs. Therefore a working installation of MPI is a 
prerequisite in order to run \PDAC\ in parallel.
The \PDAC\ serial binaries can nevertheless be run directly (this is known 
as {\it standalone} mode) for single process runs on most platforms.
Since different versions of MPI libraries are very often incompatible, you will 
likely need to recompile \PDAC\ to run it in parallel. For serial runs
the provided non-MPI binaries should still work.

\subsection{Individual Unix Workstations}

In this section we will explain how to run \PDAC\ on your own
workstation without a queuing system like
OpenPBS or Loadleveler (see below on this chapter).
On individual Unix workstations the use of the serial \PDAC\ is
quite easy, basically it behaves like any other Unix process
running on your machine.
To run \PDAC\ then follow the steps:

\begin{itemize}

\item create a running directory\
\begin{verbatim}
> mkdir sample_run
\end{verbatim}

\item copy executable (\FIL{pdac.x}) and input file (\FIL{pdac.dat})
      to the running directory\
\begin{verbatim}
> cp pdac.x pdac.dat sample_run
\end{verbatim}

\item run \FIL{pdac.x} (possibly in background)
\begin{verbatim}
> pdac.x &
\end{verbatim}

\end{itemize}

For multiprocessor workstations, if you have a working MPI
library, you can compile \PDAC\ in parallel mode to use
all processors of the workstation. In this case,
to run \PDAC\ in parallel you should use an MPI loader 
(usually distributed with the MPI library), i.e. if
your MPI library is MPICH you can run \PDAC\ using the commad:

\begin{verbatim}
> mpirun -np 2 pdac.x
\end{verbatim}

where the command parameter ``-np 2'' specifies that you want
to run on two processors.

\subsection{Server and Parallel Supercomputers}

In this section we explain how to run \PDAC\ on Unix servers
and parallel supercomputers giving you some practical example
for most popular architectures (at the time of writing).
The main differences you will find, with respect to running on 
individual workstations,
is represented by the presence of a queuing system that
manage the user requests. Usually, you should also 
consider the limits the system administrator has set to your account,
like disk space, memory or available running queues. 
If this limits are not taken into account, this could cause
the simulation not to run or complete.
Below, as anticipated, we will give you few examples on how to run
on popular servers and supercomputers, but remember that 
these systems are highly customizable and what is presented here
should be probably modified to run on other supercomputers.

\subsubsection{IBM RS/6000 servers and parallel supercomputers}

On most of IBM servers and RS/6000 supercomputers, to run a simulation you
should write a job script to be submitted to the Loadleveler which
is the IBM queuing system.
Many systems allows you to run MPI program in interactive mode for
testing and debugging, through the POE program.  
The options and environment variables for POE are various and arcane, 
so you should consult your local documentation for recommended settings.  
As an example for interactive mode runs on the RS/6000 supercomputer
(Sp4) installed at CINECA at the shell prompt you should give the
command:

\begin{verbatim}
> poe ./pdac.x -procs 4 -nodes 1 
\end{verbatim}

to run on a single node using 4 processors.
For long runs it is required that you write a job script
and submit it to the Loadleveler. Here is an example.

\begin{verbatim}
#!/bin/ksh
#@ job_type = parallel
#@ output = job.out
#@ error = job.err
#@ notification = never
#@ checkpoint = no
#@ restart = no
#@ wall_clock_limit =6:00:00
#@ resources = ConsumableCpus(1) ConsumableMemory(1024 mb)
#@ class = parallel
#@ network.MPI = csss,shared,US
#@ total_tasks = 8
#@ blocking = unlimited
#@ queue

cp pdac.dat /scratch/myrundir/pdac.dat
cp pdac.x /scratch/myrundir/pdac.x
cd /scratch/myrundir/
./pdac.x
\end{verbatim}

The above script asks for 8 processors to run 8 MPI tasks
with a limit of 1024Mbyte each for 6 hours on the parallel queue.
Finally you should submit the job (suppose the file name is \FIL{myjobfile}):

\begin{verbatim}
> llsubmit myjobfile
\end{verbatim}

\subsubsection{Linux Clusters (Beowulf)}

The most common way to run a program on a linux cluster 
is through the use of the combination of OpenPBS, MAUI scheduler
and mpirun/mpiexec MPI loaders .
For this kind of machines you could find an almost infinite number
of environments and system softwares, therefore keep the example below
as indicative, quite probably it would not work on other systems. 
As for other servers, you need to write a job script with
the request for the queuing system and with the command you 
want to execute.

\begin{verbatim}
#!/bin/sh
#PBS -l nodes=16:ppn=2,walltime=6:00:00

cp pdac.dat /myrundir
cp pdac.x /myrundir
cd /myrundir
mpiexec -np 32 ./pdac.x
\end{verbatim}

with this script we are asking for 16 nodes with 2 processors per node
and 6 hours of execution time in the queue. Then you need to
submit the job to the queuing system; for OpenPBS the command is:

\begin{verbatim}
> qsub myjobscript
\end{verbatim}

where we have supposed that the filename of the script is
\FIL{myjobscript}.

\subsubsection{HP XC Linux clusters}

This class of machines are based on ItaniumII processor nodes,
interconnected by differet kind of networks (Quadrics or Myricom),
and, like other linux cluster, the machine load (running jobs)
could by managed by OpenPBS queuing system and MAUI scheduler , 
but often on this class of machine you could fid LSF queuing and
scheduling system. 
Then here we present and example on how to run the PDAC using
LSF script:

\begin{verbatim}
#!/bin/bash 
# optional, specify the shell (/bin/sh is the default).
#BSUB -q normal         #  my preferred queue
#BSUB -o job.o -e job.e #  my  default  stdout,stderr files
#BSUB -n 6              #  the number of processors
cp pdac.dat /scratch/myrundir
cp pdac.x /scratch/myrundir
cd /scratch/myrundir
applaunch -sz 6 ./pdac.x
\end{verbatim}

with this script we are asking for 6 processors on the queue "normal",
and we specify that the standard output and error will be redirected 
to the file job.o and job.e. Then you need to
submit the job to the queuing system; for LSF the command is:

\begin{verbatim}
> bsub < myjobscript
\end{verbatim}

note that at variance with other queuing systems here the $<$ sign is used to 
to submit the job script. Again we have supposed that the filename of the script is
\FIL{myjobscript}.


\subsubsection{Compaq AlphaServer SC}

If your machine is interconnected by Quadrics network (like AlphaSC server )
you should use the 
Elan communication subsystem commands to run your parallel applications
or jobs. The main command is the \FIL{prun} command, that is similar
to mpirun of MPICH, but with prun you could run both job scripts
and interactive commands. Here is an example :

\begin{verbatim}
> prun -n 8 ./pdac.x
\end{verbatim}

this command will run \FIL{pdac.x} on 8 processors, as soon as they will
be free, infact if there are other job (or commands) already running
your command will be queued.
There are several additional options taht should be consulted on the specific 
documentation.

\subsubsection{SGI Origin and Altix systems}

SGI Origin and Altix systems are shared memory parallel machines with
real single system image framework. In other words the user
can see or access all the processors directly from the 
login environment. Differently from other architectures,
there is no distinction between login and execution nodes.
This has some benefits and drawbacks: benefits come from
the possibility to use the whole machine as a multiprocessors
workstation with high number of processors, the drawbacks
are related to the possibility that a different job running
at the same time on the machine interferes with your job.
In general, you can run parallel applications 
in two different ways, using a shell command like in your own
workstation and using a queuing system, which is usually NQS.
To run from the shell prompt use mpirun, i.e.: 

\begin{verbatim}
> mpirun -np 8 ./pdac.x
\end{verbatim}

This will execute \FIL{pdac.x} with 8 MPI task, that not necessarily
goes to different processors, it depends on the system load.
Although this way of executing parallel applications is very
friendly, it could cause a lot of interferences with system
activities and other applications, with the result of slowing
down the system. Then, quite often, the system administrator
set severe limits to this way of running, mainly on the number of
real available processors (no matter how many MPI tasks you are asking
for), execution time and memory usage.
Therefore on many systems to run large applications you need to 
interface with the NQS queuing system, so that
you have to write a script with the requests for processors and
memory and then submit the script to NQS. On most systems you
could find OpenPBS instead of NQS, in this case the job script
will be similar to the one presented above in the case of 
linux cluster, here we present an example of job script for 
NQS is the following: 

\begin{verbatim}
#!/bin/sh
#QSUB -l mpp_p=8
#QSUB -l p_mpp_t=2:00:00

cp pdac.dat /myrundir
cp pdac.x /myrundir
cd /myrundir
mpirun -np 8 ./pdac.x
\end{verbatim}

with this script you ask for 8 processors for 2 hours, then
you could submit the script with the command
(suppose its filename is \FIL{myjobfile}):

\begin{verbatim}
> qsub myjobfile
\end{verbatim}

\subsection{Grid environment}

Few simulations on a Grid environment (such as Condor) have been performed.
Some attention must be payed to the settings of MPI environment in Condor,
to the need of static compiling and to possible errors relied to the
read-write opening of the restart file. Further study will be devoted in
the next future to the possibility of running \PDAC\ on a distributed
system.

\subsection{Memory usage considerations}
The use of parallel supercomputers or PC clusters is mandatory not only
for the purpose of decreasing simulation time, but also since memory
occupation can be too high for a single workstation. As a rough estimate,
consider that, for a ``standard'' 3D simulation (with 2 solid phases) 
\PDAC\ requires a memory occupation of about 1.5 KBytes/cell. 

\subsection{Monitoring parallel speedup and scaling}

Although \PDAC\ is designed to be a scalable program, particularly for
large simulations (100,000 cells or more), increasing the number of
processors above a certain threshold will provide little or no extra performance.
This is known as ``speedup saturation'' and is
due to the increase of the ratio between the communication and computation
times. If you have access to a parallel machine you should
measure \PDAC\ s parallel speedup for a variety of processor counts when
running your particular simulation. The easiest and most accurate way
to do this is to look at the information contained in the timing report
in the Log files for the different routines. On many parallel architectures
fine-tuned performance monitoring can be performed through appropriate programs.
Note that now \PDAC\ data distribution module, could distribute domain cells
using different distribution scheme (ditribute by layers Columns and Blocks),
and different scheme have different speed-up. As a general rule, Blocks
scheme perform better for high number of processors, while Columns scheme
is better to run on few processors.
