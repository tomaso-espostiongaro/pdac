\section{Running \PDAC}
\label{section:run}

PDAC runs on a variety of serial and parallel platforms.  While it is
trivial to launch a serial program, a parallel program depends on a
platform-specific library such as MPI to launch copies of itself on
other nodes and to provide access to a high performance network such
as Myrinet if one is available.

On all parallel systems, either cluster of workstations or 
parallel supercomputers, \PDAC\ relais on the local implementation
of MPI for its communications, and standard system tools such as
mpirun are used to launch jobs. Therefore a working installation
of MPI is a prerequisite in order to run \PDAC\ .
The \PDAC\ serial binaries can also be
run directly (known as standalone mode) for single process runs,
on most platforms.
Since MPI libraries are very often
incompatible between versions, you will likely need to recompile \PDAC\
to use these machines in parallel
(the provided non-MPI binaries should still work for serial runs.)

\subsection{Individual Unix Workstations}

In this section we will explain how to run \PDAC\ on your own
workstation without a queuing system like
OpenPBS or Loadleveler ( see below on this chapter ).
On individual Unix workstations the use of serial \PDAC\ is
quite easy, basically it behaves like any other Unix process
running on your machine.
To run \PDAC\ then follow the steps:

\begin{itemize}

\item create a running directory\
\begin{verbatim}
  mkdir sample_run
\end{verbatim}

\item copy executable (pdac.x) and input file (pdac.dat)
      to the running directory\
\begin{verbatim}
  cp pdac.x pdac.dat sample_run
\end{verbatim}

\item run pdac.x ( possibly in background )
\begin{verbatim}
  pdac.x &
\end{verbatim}

\end{itemize}

Nevertheless you are the only user of your workstation,
we suggest to brokeup the simulation in shorter partial
simulations ( few days maximum each ) and use the
restart capability od \PDAC. In this way you 
reduce the probability of data/work loss due to 
unforseenable events (a system crash, a blackout, ecc...).
When restarting a simulation, remember to set the 
restart mode to "restart" and to save the pdac.res file,
in case somthing goes wrong in the new simulation step.

For multiprocessor workstations, if you have a working MPI
library, you can compile \PDAC\ in parallel mode to use
all processors of the workstations. In this case,
to run \PDAC\ in parallel you should use an MPI loader 
(usually distributed with the MPI library), i.e. if
your MPI library is MPICH you can run \PDAC\ using the commad:

\begin{verbatim}
  mpirun -np 2 pdac.x
\end{verbatim}

where the command parameter "-np 2" specify that you want
to run on two processors.

\subsection{Server and Parallel Supercomputers}

In this section we explain how to run \PDAC\ on Unix servers
and parallel supercomputing, giving you some practical example
on most popular architectures (at the time of writing).
The main differences with respect running on individual workstations
is represented by the presence of a queuing system that
manage the users rtequests. Usually you should also 
consider the limits the administrator has set to your user,
like disk space, memory or available running que. 
If this limits are not taken in to account could cause
the simulation not to run or complete.
Below, as anticipated, we give you few example on how to run
on popular server and supercomputers, but remember that 
this system are highly customizable and what is presented 
probably should be modified to run on other supercomputers.

\subsubsection{IBM RS/6000 servers and parallel supercomputers}

On most of IBM servers and supercomputers, to run your code you
should write a job script to be submitted to the Loadleveler which
is the IBM queuing system.
You can run the MPI version of \PDAC\ as you would any POE program.  
The options and environment variables for poe are various and arcane, 
so you should consult your local documentation for recommended settings.  
As an example, to run on CINECA machine you would use the following script:

\begin{verbatim}
\end{verbatim}


\subsubsection{Linux Clusters (Beowulf)}

The most common way to run program on a linux cluster 
is through the use of the combination of OpenPBS, MAUI scheduler
and mpirun/mpiexec loaders .
For this kind of machine you could find an almost infinite number
of environments and system softwares, therefore keep the example below
as an indication, most probably it wan't work on other system. 

\begin{verbatim}
\end{verbatim}

For best performance, ...

\subsubsection{Compaq AlphaServer SC}

If your machine as a Quadrics interconnect you should use the Elan
version of \PDAC\, other wise select the normal MPI version.  In either
case, parallel jobs are run using the ``prun'' command as follows:

\begin{verbatim}
\end{verbatim}

There are additional options.  Consult your local documentation.

\subsubsection{Origin 2000}

For small numbers of processors (1-8) use the non-MPI version of PDAC2.
If your stack size limit is unlimited, which DQS may do, you will need
to set it with ``limit stacksize 64M'' to run on multiple processors.
To run on <procs> processors call the binary directly with the +p option:

\begin{verbatim}
  PDAC2 +p<procs> <configfile>
\end{verbatim}

\begin{verbatim}
  setenv MPI_REQUEST_MAX 10240
  setenv MPI_TYPE_MAX 10240
\end{verbatim}

Then run PDAC with the following command:

\begin{verbatim}
  mpirun -np <procs> PDAC2 <configfile>
\end{verbatim}

\subsection{Grid environment}

\subsection{Memory Usage}

.... Memory usage consideration ...

\subsection{Improving Parallel Scaling}

.... Choosing the correct number of processors

While \PDAC\ is designed to be a scalable program, particularly for
large simulations ( 100,000 cells or more), at some point adding additional
processors to a simulation will provide little or no extra performance.
If you are lucky enough to have access to a parallel machine you should
measure \PDAC\'s parallel speedup for a variety of processor counts when
running your particular simulation.  The easiest and most accurate way
to do this is to look at the ``Prog'' time printed in the "pdac.dat" file.
You can monitor performance during the entire simulation looking at
the "walltime" line in the "pdac.log" file.

Extremely short cycle lengths (less than 10 steps) will also limit
parallel scaling, since the ghost cells migration at the end of each cycle
sends too many messages.
