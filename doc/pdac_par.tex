\section{Parallel Implementation}
%
The parallelization strategy adopted in PDAC is based on a domain decomposition
of the real space grid using a SIMDA (Single Instruction Multiple DAta) paradigm
within a message passing scheme. This technique consists in the decomposition
of the numerical mesh among N processors. Each processor solves the whole set of
the model equations (Single Program) on its local data (Multiple Data),
that are not accessible to other processors.
%
The parallel software layer in the code has been designed trying to make it as much 
hidden as possible, so that the resulting code, a part few control constructs,
looks like a serial code, and basic algorithms that solve the constituent equations
have not been modified.
The design kyefeature of the parallelization has been the implementation of a 
low level library, which is used
to wrap MPI, and of a high level f90 module that contains subroutines
to distribute computational cells among processors, balancing the load, and to build the 
messages maps, to exchange data across processors.  
Here the details of the parallel implementation of PDAC are
presented following the main logical steps:
\begin{enumerate}
\item domain decomposition
\item construction of processors maps
\item setting of indexes
\item construction of the receive maps and send maps
\item data exchange
\item communication interfaces
\end{enumerate}
% 
\subsection{Domain decomposition}
The computational domain
is discretized into a structured rectilinear mesh, so that the total number
of cells is given, considering the ground in the cartesian xy plane,  
by the number of cells in x direction times the number of cells in z direction
for the 2D case, and by the number of cells in x direction times the number of cells 
in the y direction times the number of cells in z direction, for the 3D case.
The parallelization is obtained distributing the
computational cells among processors, in such a way that each processor solve
the coinstituent equations only on its own cells. 
Several distribution scheme are possible, and the different computational weight 
of cells in different part of the domain need to be accounted for.
Solid obstacles or topographic reliefs on a structured rectilinear mesh are 
approximated by introducing {\em blocking cells}, where solid boundary 
conditions are specified and fluid flow is not computed.
Note that in the case of a large number of blocking cells, 
a regular domain decomposition would result into an unbalanced load.\\
In PDAC, cells are subdivided into five classes:
\begin{itemize}
\item fluid cells, where all the model equations are solved according to the described solution precedure (flag = 1);
\item axial cells (flag = 2) and solid walls (flag = 3), where the velocity 
field is imposed according with the assumed boundary conditions; 
\item atmospheric boundary cells (flag = 4), where free in/out-flow 
conditions are accounted for by solving a reduced set of differential 
equations, specifically the transport mass and momentum balance equations 
for the mixture;
\item specified fluid cells (flag = 5), where the pressure, temperature, 
volume fraction and velocity fields are specified for each phase.
\end{itemize}

The domain decomposition algorithm assignes a computational weight to each type 
of cells and subdivides the computational domain
among $N$ processors so that each sub-domain has the same total weight.
A weight of 1 is assigned to fluid cells (flag = 1),
while a weight of 0 is assigned to the other types (flag = 2,3,4,5). 
The domain decomposition algorithm then subdivides the domain so that each processor
has the same number of cells of type 1, following these main steps:.\\
\begin{enumerate}
\item Counts the number of cells of each type, and subdivides the number $F1$ of ``flag 1 cells''
 among the number of processors $N$.
\item {\bf Layers} decomposition (both 2D and 3D domain): the mesh is decomposed into horizontal 
layers with equal total number
$F1$ of ``flag-type 1'' cells (Fig.~\ref{fig:Ves_part}, left). The procedure simply sweeps across
the grid from bottom to top and counts the number of computational cells in each row from
left to right (main sweep). Cells are assigned to a single process until the correct number 
of sub-domain cells $F1$ is reached, then the process rank is increased until every 
process has an equal number of computational cells.
\item {\bf  2D Blocks } decomposition: tries to subdivide the mesh
into rectangular blocks with a shape factor near one (approximately squared) 
(Fig.~\ref{fig:Ves_part}, right). The procedure first determines the best number
of vertical layers $n_z$ along the $z$ cartesian axis, then within each 
layer find out the optimal number and size of blocks, to be assigned to processors.
\begin{enumerate}
\item computes $\displaystyle n_x = NINT\left(\frac{ndi}{\sqrt(F1/N)}\right)$, where $ndi$ is the
number of grid cells in the $x$ direction;
\item compute the number of layers $n_z$ so that $n_x \times n_z \ge N$ 
( $\displaystyle n_z = NINT\left(\frac{N}{n_x}\right)$ );
\item if $n_x \times n_z = N$ each layer will be subdivided into $n_x$ blocks, and each layer
will have the same number $F1$ of computational cells. If $n_x \times n_z > N$, the excess blocks
( $n_x \times n_z - N$ ) are subtracted from layers containing the topography, and this time
the number of $F1$ cells in each layer is set to be proportional to the number of blocks
assigned to the layer itself. 
Once the number of $F1$ cells per layer has benn computed the height is computed 
proceeding across the grid following the main sweep (as in Layer decomposition);
\item finally, each layer is subdivided horizontally into the previously computed number
of blocks, and each blocks is assigned to a processor. 
\end{enumerate}
\item {\bf 3D Columns } decomposition: same algorithm of the 2D blocks but applied to the $xy$ 
ground plane instead of $xz$ vertical plane, the third cartesian axis $z$ is not subdivided
further. (Fig.~\ref{fig:Ves_part}, right) and the the 3D domain will result to be subdivided
into columns whose height is $nz$ (size of the mesh in the $z$ direction) and whose base
is approximately square.
\item {\bf 3D Blocks} decomposition: first all the number of subdivision to be done
along the $z$ vertical $n_z$ is computed requiring that $n_z$ being as much as possible
close to $N^{1/3}$ and $n_z$ being a factor of $N$ ( $N/n_z = INT(N/n_z)$ ).
Then the algorithm proceed as for 3D columns partitioning the $xy$ plane, 
but substituting $N$ with $N/n_z$. Finally all columns are cutted into $n_z$ blocks.
\end{enumerate}

As explained blelow the amount of communications required for the solution 
of the constituent equations
are proportional to the cells that are at the boundary between sub-domains assigned to
different processors, and can be estimated to scale as $N$ for the layers decomposition, 
$N^{1/2}$ for the 3D columns
and 2D blocks decomposition, and $N^{1/3}$ for 3D blocks decomposition.\\
Therefore different decompositions are expected to display different speed-ups
(increase of performances) when the number of processors used for a single run
is increased. In particular layers decompositions is expected to scale better
for low processors count and blocks decompositions are expeted to perform 
better for high processors count.

%
\begin{figure}[htb]
\setlength{\unitlength}{1mm}
\begin{center}
\begin{minipage}{6cm}
\centerline{\psfig{figure=./layer_dist.eps,height=4.5 cm,angle=0}}
\end{minipage}
\begin{minipage}{6cm}
\centerline{\psfig{figure=./column_dist.eps,height=4.5 cm,angle=0}}
\end{minipage}
\begin{minipage}{6cm}
\centerline{\psfig{figure=./block_dist.eps,height=4.5 cm,angle=0}}
\end{minipage}
\end{center}
\caption{Example of domain decompositions with balanced distribution of
the fluid cells in four blocks (left) or four layers (right).
The domain is axisymmetric so that the figure represents a poloidal
half-plane with the simmetry axis on the left hand-side boundary and
the volcano topography at the bottom. In these figures the cells are represented
with a uniform size so that the areas of the computational sub-domains
are equal.\label{fig:Ves_part}}
\end{figure}
%

Communication between processors allows the exchange of the flow variables
at the boundaries between sub-domains and the convergence of the solution
in the global domain (it is worth noting that the
boundary data exchange is required at each iteration of the point-by-point
procedure solving the implicit algebraic system of PDE).
Boundary values of neighbour processors are allocated in a virtual
frame surrounding the processor sub-domain (ghost-cells). This technique
enables the reduction of the communication time with a relatively low cost
in terms of memory storage.
In the next sections, a more detailed explanation of the implementation of communication
routines is presented.
%
\subsection{Processor-maps}
A processor-map is a function that, for each index in the main grid, returns the rank
of the processor that owns the corresponding cell, and the index that identifies the
cell in the local sub-domain.

In PDAC any domain decomposition could be in principle implemented, provided that a 
simple enough processor-map can be designed.
For layers (both in 2D and 3D domain), the processor map is simply given by a couple of integers,
representing the indexes of the first and the last cells of the layer, following
the main sweep 1D arrangement.  Each process can thus recover
the informations on the domain decomposition if it knows all processor maps.
For 2D blocks ( 3D columns ) one needs the 2D coordinates (two indexes) of the left-bottom ( south-west )
top-right ( north-east ) corner of the sub-domains.
For 3D blocks one needs the 3D coordinates (three indexes) of the bottom-south-west and
top-north-east corner of the sub-domains.

The processors map allows a fast and memory-cheap way to define the relations
between the global index, the number of processor and the local index 
within the sub-domain, for a given cell.\\
%
\subsection{Mesh indexes}
Cells within a sub-domain are ordered as in the main sweep, from bottom to top and
from left to right ( from west to east, south to north and bottom to top in 3D case). 
{\tt nijk\_l} is the (local) number of cells in a subdomain ( k always 1 for 2D domain).
As a first step, each processor counts the total number of external ghost cells
needed during a computational cycle (the identification of the ``owner'' is 
possible thanks to processor maps). If {\tt nijke\_l} is the total number of these 
cells, it must allocate an array of dimension {\tt nijke\_l} to store the data received
from its neighbours. Ghost cells are ``appended'' to the local arrays:
the {\tt myijk} and {\tt myinds} arrays are implemented
 to store and access values allocated in the ghost cells and to promptly
identify cells neighbours when needed for computation.\\
The array {\tt myijk} is built as a matrix with rank=2 and size 
$nb \times {\tt nijk\_l}$ where $nb$ is the number of neighbour cells 
needed for computation and {\tt nijk\_l} is the dimension of each processor
sub-domain.\\
The first index of the matrix runs over the neighbour identified by a progressive
index $in$ that run from 1 to 9 ( in 2D domain ) and from 1 to 25 ( in 3D domain ),
note that in the code mnemonic variable are used in place of number to address
the neigbour (ex. ip1\_jp0\_km1\_ $=$ 20 is the neighbour $i+1,j+0,k-1$ of the $i,j,k$
cell).\\ 
Computation proceeds by looping  over the computational
cell of one processor's sub-domain: for each cell, if $ijk$ is its local index
we can write
\begin{enumerate}
\item ${\tt myijk}( 1,ijk)$ gives the GLOBAL index (the index running 
      over the main grid) corresponding to the LOCAL index $ijk$;
\item ${\tt myijk}( im,ijk)$ gives the LOCAL index that identifies the neighbour
      cell identified by the mnemonic index $im$, with respect to $ijk$. 
\end{enumerate}

On boundaries, the number and type of neighbour cells that must be considered
in the computation is different from the internal cells. The array
{\tt myinds} duplicates the array {\tt myijk} (although it has a different notation)
in the internal cells and considers boundary conditions on boundaries.\\ 
%
\subsection{Receive and send-maps}
%
Once indexes have been set, every processor builds up a map of the ghost cells
to ask to the neighbour processors. The {\tt receive\_map} is 
defined as a new variable type from which each processor can obtain the following informations:
\begin{enumerate}
\item the number of elements he must receive from a given neighbour processor;
\item the GLOBAL indexes of all these elements;
\item the LOCAL indexes to be assigned to these elements.
\end{enumerate}
Each processor allocates the {\tt receive\_map} as an array with an entry for every 
external processors. It then scatters the array to send a request to other 
processes for those indexes contained in the receive-map.\\
After each processor has received the request from the others,
it sets out the relative send\_maps. The {\tt send\_map} is defined as a new variable
type, containing the following elements:
\begin{enumerate}
\item the number of elements he must send to a given neighbour processor;
\item the GLOBAL indexes of these elements;
\item the LOCAL indexes of these elements in my sub-domain.
\end{enumerate}
Each processor allocates an array of send\_maps with an entry for each external processor.
%
\subsection{Data exchange}
%
Buffers for the standard blocking {\em send-receive} MPI procedure are filled 
accordingly to the {\tt send\_map} and {\tt receive\_map} for each processor.\\
The processors are put in increasing order from 0 to $N-1$, with processor
0 following processor $N-1$ and preceeding processor 2. 
The {\tt data\_exchange} requires $N-1$ communication step: at the first step the generic 
processor $P$ sends data to $P+1$ and receives data form $P-N+1$;
at the second step it sends data to $P+2$ and receives from $P-N+2$; 
at the $(N-1)$th step it send data to $P+N-1=P-1$ and receives data from $P-N+(N-1)=P+1$.\\ 
In fig. \ref{fig:data-exchange} the communication scheme is sketched for $N=5$.\\
\begin{figure}[h]
\centerline{\psfig{figure=./exchange.eps,height=5cm}}
\caption{\label{fig:data-exchange} Communication steps in {\tt data\_exchange} for 5 processors}
\end{figure}
\par
%
\subsection{Communication interfaces}
%
\subsubsection{Data exchange}
%
The {\tt data\_exchange} interface is set so that each processor exchanges
data with others calling the {\em send-receive} MPI routine.\\ 
Once the communication scheme is set, the {\tt data\_exchange} interface 
is completely transparent, providing a direct way to store and access values 
allocated in the ghost-cells and allowing the user to somehow forget the 
nature of the communication itself. 
The call to the data exchange procedure is overloaded, so that it can be used
for any variable type.  When a variable ({\em array}) is
 updated, if its value must be exchanged at boundaries, the communication is invoked by
simply calling:
\begin{displaymath}
\tt CALL \quad data\_exchange({\it array})
\end{displaymath}
\clearpage%
